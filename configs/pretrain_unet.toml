[general]
use_wandb = 1
seed = 0
style = 'unet'
d_adapter = 1024
val_size = 4800
normalize_embeddings = true
depth = 2
embs = ['gte', 'gtr', 'stella', 'sentence-t5', 'e5', 'snowflake', 'ember', 'sbert']
lm_base_name = "google-bert/bert-base-uncased"
upscale_num = 16
n_embs_per_batch = 2
max_seq_length = 32

[train]
lr = 1e-4
bs = 256
save_every = 6_000
epochs = 0.5
val_bs = 64
dataset = "fineweb-medium"
max_grad_norm = 10.0
gradient_accumulation_steps = 2
loss_coefficent_rec = 1
loss_coefficient_trans = 1
loss_coefficient_vsp = 0
loss_coefficient_cc = 0
add_noise = 1
add_noise_rotation = 0

[logging]
wandb_project = 'edx-1'
wandb_name = 'unet-pretrain'
save_dir = 'checkpoints-unet/{}/'

[huggingface]
hf_username = 'rishi-jha'
model_name = 'unet-pretrain'

[eval]
dataset = "nq"
val_folder = 'results/'
val_size = 10000
inversion_size = 1000
val_bs = 64
text_embs = ['gtr']
