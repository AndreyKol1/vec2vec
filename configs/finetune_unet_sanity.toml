[general]
use_wandb = 1
seed = 0
style = 'unet'
d_adapter = 1024
val_size = 1024
normalize_embeddings = true
depth = 3
embs = [ "gte", "gtr", "stella", "sentence-t5", "e5", "snowflake", "ember", "sbert", "gist", "clip", "bert-nli",]
lm_base_name = "google-bert/bert-base-uncased"
upscale_num = 16
n_embs_per_batch = 2
max_seq_length = 512

[train]
ft_embs = ['sbert', 'ember']
src_emb = 'sbert'
tgt_emb = 'ember'
overwrite_emb = 'sbert'
lr = 1e-4
bs = 256
save_every = 6_000
epochs = 10
val_bs = 256
dataset = "nq"
max_grad_norm = 10.0
gradient_accumulation_steps = 2
loss_coefficent_rec = 0
loss_coefficient_trans = 1
loss_coefficient_vsp = 0
loss_coefficient_cc = 0
add_noise = 0
add_noise_rotation = 0
force_dump = true
overwrite = false
freeze_params = true
patience = 1
delta = 0.0

[logging]
wandb_project = 'edx-v1-finetune-nq-sanity2'
wandb_name = 'edx-v1-finetune'
load_dir = '/opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/jxm/supervised_translation/checkpoints-unet/edx-v1/model.pt'
save_dir = '/opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/jxm/supervised_translation/checkpoints-test/{}/'
